{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9127738,"sourceType":"datasetVersion","datasetId":5510810}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n# ------------\n\ntorch.manual_seed(1337)\nfile_path = '/kaggle/input/literature/literature.txt'\n# Read the local file\nwith open(file_path, 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\"one head of self-attention\"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        \n        # compute the attention scores \n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n\n        # perform weighted aggregation of the values\n        v = self.value(x)\n        self_attention = wei @ v\n        return self_attention \n    \nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel -> one of the main advantages of transformers over LSTM or other RNNs. \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim = -1)\n        out = self.dropout(self.proj(out))\n        return out\n        \nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4*n_embd),\n            nn.ReLU(),\n            nn.Linear(4*n_embd, n_embd),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: commication followed by computation \"\"\"\n    def __init__(self, n_embd, num_head):\n        super().__init__()\n        head_size = n_embd // num_head\n        self.sa = MultiHeadAttention(num_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n    \n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n    \"\"\" GPT Language model \"\"\"\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)  \n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = GPTLanguageModel()\nm = model.to(device)\n\n# create pytorch optimizer \noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-08T18:58:58.072305Z","iopub.execute_input":"2024-08-08T18:58:58.072669Z","iopub.status.idle":"2024-08-08T19:28:34.793805Z","shell.execute_reply.started":"2024-08-08T18:58:58.072633Z","shell.execute_reply":"2024-08-08T19:28:34.792772Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"step 0: train loss 4.8166, val loss 4.8184\nstep 500: train loss 1.8113, val loss 1.7527\nstep 1000: train loss 1.4423, val loss 1.3815\nstep 1500: train loss 1.3136, val loss 1.2674\nstep 2000: train loss 1.2519, val loss 1.2071\nstep 2500: train loss 1.2085, val loss 1.1727\nstep 3000: train loss 1.1809, val loss 1.1500\nstep 3500: train loss 1.1519, val loss 1.1273\nstep 4000: train loss 1.1406, val loss 1.1166\nstep 4500: train loss 1.1185, val loss 1.1059\nstep 4999: train loss 1.1064, val loss 1.0976\n\nThe best could keep up years or answer in the eyes;\nSwering ago. More to your girls in deal is.\n\nPROSPERO:\nYes, how you will shall still go gods?\nHave so when you're your highness, anielosopher,\nDoubtled jumbery with Petersbury\nLies why I loafter'd Rome.\nLeave the law, Clarers applaid.\nI, since, is't, this thrifted stain'd!\nWe may make you; go, get well in;\nThe Earl of Lanca.\n\nDUCHESS OF YORK:\nStrant my mranisacts, and Plato!\n\nKING HENRY VINCENTIO:\nBut that KINGHAM:\nSirrah might be hath kick'd, \n","output_type":"stream"}]},{"cell_type":"code","source":"print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:34:11.942204Z","iopub.execute_input":"2024-08-08T19:34:11.942585Z","iopub.status.idle":"2024-08-08T19:34:20.110959Z","shell.execute_reply.started":"2024-08-08T19:34:11.942555Z","shell.execute_reply":"2024-08-08T19:34:20.110059Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\n“Only could not evers merely have benother too further, so as to his behavious, in anfied, a fact, it’s enough. And you are against that was not all this matters. So for some slap though it’s all seemed to be something. In the laughter of the words it ought to have who wrkneed your last years with you, estting right, flurted verying young, themselves in at your blue without it. I kiss every living Roman evening, Sofya,, at Swellove ousy, show it two of hungers, am possible nowable, spirious. Fas\n","output_type":"stream"}]},{"cell_type":"code","source":"print(decode(m.generate(context, max_new_tokens=1500)[0].tolist()))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:36:02.938480Z","iopub.execute_input":"2024-08-08T19:36:02.939176Z","iopub.status.idle":"2024-08-08T19:36:27.634326Z","shell.execute_reply.started":"2024-08-08T19:36:02.939124Z","shell.execute_reply":"2024-08-08T19:36:27.633191Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\nin steps at them before it. Outside every voice Romans.\n\n—Who yes? cross, tellous here, handly CORIOLES\nChurch? No. Conspicuo: tell her ark. Poleise there.\n\nShe his tonguets and persons. Sax to the whitest shade. Namely anon. Then he begged the table.\n\nMitya was celly wring to himself. In a new cheaper Ilyrane. The twines, he prisoned. Mrs Bloom in tears the heaper. A curly fluel steading at all an illness which Stephen. But it was knocked the shudder. Sonia forced with shirt salutters, toc! Still, he quickly, as though he usuated to Grn: All Gania’s curts several brack the thray. He ceased glling undo eavely around him; and he smiled with a loused micked and filled his shagging. He ran toward his where, they listened in the stairs and he took his head.\n\n\n“He hasn’t been obligent at once tell meetinks indulges, but Burdovsky,” pale off, with its next right son and persisted himself out of it.\n\n\n“How are you chan’t sat her, when she did,” asked, grew himself and angry and unknocking the right handking Raskolnikov. She was expecting that he flushed Razumihin really at the sofa. “Well, you must start it! Having then I begin about a very shutter-a tack it. That’s been down to cry, I saw my ready.”\n“It’s only you’ve felt an idea,” she exclaimed wearily as quite struck her hen and Dounia the Pole. “I did not confess at once to happen for a mixed thoughts, of course, for every one, to publish the others a step of three behind whether Porfiry, he was beingered, strange among the Whol\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}